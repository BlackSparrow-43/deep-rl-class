{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install gym[atari] autorom[accept-rom-license]\n",
    "!pip install pybullet\n",
    "!pip install ale-py\n",
    "!pip install -U ipywidgets\n",
    "!pip install gymnasium[atari,accept-rom-license]\n",
    "!pip install gputil\n",
    "!pip install gym[classic_control]\n",
    "!sudo apt-get install xvfb\n",
    "!pip install pyvirtualdisplay Pillow\n",
    "!pip install huggingface_sb3\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install --user --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ID: 0, GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU Load: 0.0, GPU Free Memory: 5996.0\n",
      "GPU Total Memory: 6144.0, GPU Temperature: 57.0\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(\"GPU ID: {}, GPU Name: {}\".format(gpu.id, gpu.name))\n",
    "    print(\"GPU Load: {}, GPU Free Memory: {}\".format(gpu.load, gpu.memoryFree))\n",
    "    print(\"GPU Total Memory: {}, GPU Temperature: {}\".format(gpu.memoryTotal, gpu.temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95eda70b703e651a93829d62187e30b7bc107306\n",
    "\n",
    "import wandb\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, push_to_hub\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"env_name\": \"PongNoFrameskip-v4\",\n",
    "    \"num_envs\": 8,\n",
    "    \"total_timesteps\": int(10e6),\n",
    "    \"seed\": 4089164106,    \n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"HFxSB3\",\n",
    "    config = config,\n",
    "    sync_tensorboard = True,  # Auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym = True, # Auto-upload the videos of agents playing the game\n",
    "    save_code = True, # Save the code to W&B\n",
    "    )\n",
    "\n",
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multi-worker training (n_envs=8 => 8 environments)\n",
    "env = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #PongNoFrameskip-v4\n",
    "\n",
    "print(\"ENV ACTION SPACE: \", env.action_space.n)\n",
    "\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "# Video recorder\n",
    "env = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n",
    "\n",
    "# https://github.com/DLR-RM/rl-trained-agents/blob/10a9c31e806820d59b20d8b85ca67090338ea912/ppo/PongNoFrameskip-v4_1/PongNoFrameskip-v4/config.yml\n",
    "model = PPO(policy = \"CnnPolicy\",\n",
    "            env = env,\n",
    "            batch_size = 256,\n",
    "            clip_range = 0.1,\n",
    "            ent_coef = 0.01,\n",
    "            gae_lambda = 0.9,\n",
    "            gamma = 0.99,\n",
    "            learning_rate = 2.5e-4,\n",
    "            max_grad_norm = 0.5,\n",
    "            n_epochs = 4,\n",
    "            n_steps = 128,\n",
    "            vf_coef = 0.5,\n",
    "            tensorboard_log = f\"runs\",\n",
    "            verbose=1,\n",
    "            )\n",
    "    \n",
    "model.learn(\n",
    "    total_timesteps = config[\"total_timesteps\"],\n",
    "    callback = [\n",
    "        WandbCallback(\n",
    "        gradient_save_freq = 1000,\n",
    "        model_save_path = f\"models/{run.id}\",\n",
    "        ), \n",
    "        CheckpointCallback(save_freq=10000, save_path='./pong',\n",
    "                                         name_prefix=config[\"env_name\"]),\n",
    "        ]\n",
    ")\n",
    "\n",
    "model.save(\"ppo-PongNoFrameskip-v4.zip\")\n",
    "push_to_hub(repo_id=\"ThomasSimonini/ppo-PongNoFrameskip-v4\", \n",
    "    filename=\"ppo-PongNoFrameskip-v4.zip\",\n",
    "    commit_message=\"Added Pong trained agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\cuda_11_8\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "d:\\Envs\\cuda_11_8\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:751: UserWarning: You are probably loading a model saved with SB3 < 1.7.0, we deactivated exact_match so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/issues/1233 for more info). Original error: Error(s) in loading state_dict for ActorCriticCnnPolicy:\n",
      "\tMissing key(s) in state_dict: \"pi_features_extractor.cnn.0.weight\", \"pi_features_extractor.cnn.0.bias\", \"pi_features_extractor.cnn.2.weight\", \"pi_features_extractor.cnn.2.bias\", \"pi_features_extractor.cnn.4.weight\", \"pi_features_extractor.cnn.4.bias\", \"pi_features_extractor.linear.0.weight\", \"pi_features_extractor.linear.0.bias\", \"vf_features_extractor.cnn.0.weight\", \"vf_features_extractor.cnn.0.bias\", \"vf_features_extractor.cnn.2.weight\", \"vf_features_extractor.cnn.2.bias\", \"vf_features_extractor.cnn.4.weight\", \"vf_features_extractor.cnn.4.bias\", \"vf_features_extractor.linear.0.weight\", \"vf_features_extractor.linear.0.bias\".  \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the libraries\n",
    "import os \n",
    "\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, push_to_hub\n",
    "\n",
    "# Load the model\n",
    "checkpoint = load_from_hub(\"ThomasSimonini/ppo-PongNoFrameskip-v4\", \"ppo-PongNoFrameskip-v4.zip\")\n",
    "\n",
    "# Because we using 3.7 on Colab and this agent was trained with 3.8 to avoid Pickle errors:\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "        }\n",
    "\n",
    "#model = PPO.load(checkpoint, custom_objects=custom_objects)\n",
    "\n",
    "model = PPO.load(r\"D:\\Projects\\Clg\\RL\\Agents\\Pong\\models\\0r8nblin\\model.zip\")\n",
    "\n",
    "env = make_atari_env('PongNoFrameskip-v4', n_envs=1)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env.metadata['render_fps'] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils.play import play\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "mapping = {(pygame.K_UP,): 2, (pygame.K_DOWN,): 3}  # pong\n",
    "\n",
    "env = gym.make('PongNoFrameskip-v4', render_mode='rgb_array')  # Use the newer version and set render_mode to rgb_array\n",
    "\n",
    "# Utilize the play function to start the environment\n",
    "play(env, zoom=3, fps=30,  keys_to_action=mapping)\n",
    "\n",
    "# Remember to close the environment when done\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Envs\\cuda_11_8\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "width, height = 210, 160  # Adjust based on your environment's frame size\n",
    "zoom_factor = 4\n",
    "screen = pygame.display.set_mode((width * zoom_factor, height * zoom_factor))\n",
    "\n",
    "obs = env.reset()\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "score = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    \n",
    "    # Render the environment's state to the Pygame window\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    frame = pygame.surfarray.make_surface(frame)\n",
    "    frame = pygame.transform.rotate(frame, -90) \n",
    "    frame = pygame.transform.flip(frame, True, False)\n",
    "    screen.blit(pygame.transform.scale(frame, (width * zoom_factor, height * zoom_factor)), (0, 0))\n",
    "    pygame.display.flip()\n",
    "    \n",
    "    # Handle Pygame events\n",
    "    for event in pygame.event.get():\n",
    "           if event.type == pygame.KEYDOWN:\n",
    "               if event.key == pygame.K_q:\n",
    "                   done = True  # Exit if 'q' is pressed\n",
    "                   break\n",
    "    # Control the frame rate\n",
    "    clock.tick(60)\n",
    "\n",
    "    score += rewards\n",
    "# Clean up\n",
    "pygame.quit()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "obs = env.reset()\n",
    "plt.figure(figsize=(5, 4))\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    img = env.render(mode='rgb_array')\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

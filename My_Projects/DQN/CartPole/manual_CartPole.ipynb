{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackSparrow-43/deep-rl-class/blob/main/My_Projects/DQN/CartPole/manual_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5u-u4d07ZQ0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, clone_model, load_model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np \n",
        "import time\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smT71bCvbE5d"
      },
      "outputs": [],
      "source": [
        "class Dqn():\n",
        "    \n",
        "    def __init__(self,Load_model = None):\n",
        "                \n",
        "        self.Q_model = self.create_model()\n",
        "        self.T_model = self.create_model()\n",
        "        #self.tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}-{}\".format(\"rl_cartpole\", int(time.time())),histogram_freq=1)\n",
        "        self.epsilon = 1.0\n",
        "        self.max_epsilon = 1.0\n",
        "        self.min_epsilon = 0.009\n",
        "        self.decay_rate = .01\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 100\n",
        "        self.replay_mem_size = 50_000\n",
        "        self.update_T_model = 10\n",
        "        self.EPOCH = 150\n",
        "        self.log_interval = 10\n",
        "        self.replay_mem = deque(maxlen=self.replay_mem_size)\n",
        "        #self.Load_model = Load_model\n",
        "\n",
        "  \n",
        "    def create_model(self,verbose=0):\n",
        "\n",
        "        model = Sequential()\n",
        "        \n",
        "        model.add(Dense(16,  input_shape=(1, env.observation_space.shape[0])))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Dense(32))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Dense(env.action_space.n))\n",
        "        model.add(Activation('linear'))\n",
        "        \n",
        "        model.compile(loss = \"mse\", optimizer = Adam(lr = .001), metrics = [\"accuracy\"])\n",
        "        \n",
        "        if verbose == 1:\n",
        "            print(model.summary())\n",
        "        return model\n",
        "    \n",
        "\n",
        "    def replay_buffer(self,epoch):\n",
        "        \n",
        "        if len(self.replay_mem) < self.batch_size :\n",
        "            return\n",
        "        \n",
        "        minibatch = random.sample(self.replay_mem, self.batch_size)\n",
        "        \n",
        "        current_states = np.array([history[0] for history in minibatch])\n",
        "        cur_state_q_list = self.Q_model.predict(current_states)\n",
        "        \n",
        "        next_states = np.array([history[3] for history in minibatch])\n",
        "        next_state_q_list = self.T_model.predict(next_states)\n",
        "        \n",
        "        features = []\n",
        "        labels = []\n",
        "        for index,(current_state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            \n",
        "            if not done:\n",
        "                max_q_value = np.max(next_state)\n",
        "                new_q = reward + self.gamma*max_q_value\n",
        "            else:\n",
        "                new_q = reward\n",
        "            \n",
        "            current_q_state = cur_state_q_list[index]\n",
        "            current_q_state[0][action] = new_q\n",
        "\n",
        "            features.append(current_state)\n",
        "            labels.append(current_q_state)\n",
        "        self.Q_model.fit(np.array(features)/255, np.array(labels), batch_size=self.batch_size, verbose=0, shuffle=False )\n",
        "        \n",
        "    \n",
        "\n",
        "    def update_model_handler(self,epoch):\n",
        "        if epoch > 0 and epoch % self.update_T_model == 0:\n",
        "            self.T_model.set_weights(self.Q_model.get_weights())\n",
        "    \n",
        "            \n",
        "    def epsilon_greedy_action_selection(self, epsilon, obs):\n",
        "        select=None\n",
        "        if np.random.random() > self.epsilon:\n",
        "            prediction = self.Q_model.predict(np.array(obs).reshape(-1, *obs.shape)/255)  # perform the prediction on the observation\n",
        "            action = np.argmax(prediction)  \n",
        "            select=\"from_table\"# Chose the action with the higher value\n",
        "        else:\n",
        "            action = np.random.randint(0, env.action_space.n)  # Else use random action\n",
        "            select=\"random\"\n",
        "        \n",
        "        return action, select\n",
        "    \n",
        "    \n",
        "    def epsilon_reduce_exp(self,epoch):\n",
        "        return (self.min_epsilon+(self.max_epsilon-self.min_epsilon)*np.exp((-self.decay_rate)*epoch))\n",
        "    \n",
        "    \n",
        "    def reward_system(self, reward, obs, done, step):\n",
        "    \n",
        "        angle = obs[0][2]\n",
        "        pos = obs[0][0]\n",
        "        reward_step = 0  \n",
        "        \n",
        "        if -6 <= angle < -5:\n",
        "            reward_ang = -60\n",
        "        elif -5 <= angle < -4.5:\n",
        "            reward_ang = -48\n",
        "        elif -4.5 <= angle < -4:\n",
        "            reward_ang = -36\n",
        "        elif -4 <= angle < -3.5:\n",
        "            reward_ang = -24\n",
        "        elif -3.5 <= angle < -3:\n",
        "            reward_ang = -16\n",
        "        elif -3 <= angle < -2.5:\n",
        "            reward_ang = -8\n",
        "        elif -2.5 <= angle < -2:\n",
        "            reward_ang = -4\n",
        "        elif -2 <= angle < -1.5:\n",
        "            reward_ang = -2\n",
        "        elif -1.5 <= angle < -1:\n",
        "            reward_ang = -1\n",
        "        elif -1 <= angle < -.5:\n",
        "            reward_ang = 0\n",
        "        elif -0.5 <= angle < 0.5:\n",
        "            reward_ang = 1\n",
        "        elif 1 <= angle < 1.5 :\n",
        "            reward_ang = 0\n",
        "        elif 1.5 <= angle < 2:\n",
        "            reward_ang = -1\n",
        "        elif 2 <= angle < 2.5:\n",
        "            reward_ang = -2\n",
        "        elif 2.5 <= angle < 3:\n",
        "            reward_ang = -4\n",
        "        elif 3 <= angle < 3.5:\n",
        "            reward_ang = -8\n",
        "        elif 3.5 <= angle < 4:\n",
        "            reward_ang = -12\n",
        "        elif 4 <= angle < 4.5:\n",
        "            reward_ang = -18\n",
        "        elif 4.5 <= angle < 5:\n",
        "            reward_ang = -26\n",
        "        elif 5 <= angle < 5.5:\n",
        "            reward_ang = -36\n",
        "        elif 5.5 <= angle < 6:\n",
        "            reward_ang = -44\n",
        "        elif 6 <= angle < 6.5:\n",
        "            reward_ang = -56\n",
        "        elif 6.5 <= angle < 7:\n",
        "            reward_ang = -64\n",
        "        else:\n",
        "            reward_ang = -100\n",
        "\n",
        "\n",
        "        if -3 <= pos < -2 :\n",
        "          reward_pos = -1\n",
        "        elif -2 <= pos < -1:\n",
        "          reward_pos = 0\n",
        "        elif -1 <= pos < 1:\n",
        "          reward_pos = 1\n",
        "        elif 1 <= pos < 2:\n",
        "          reward_pos = 0\n",
        "        elif 2 <= pos < 3:\n",
        "          reward_pos = -1\n",
        "        else:\n",
        "          reward_pos = -5\n",
        "\n",
        "        if done and step < 300:\n",
        "            reward_step = -300\n",
        "    \n",
        "        return reward_ang + reward_pos  + 1\n",
        "    \n",
        "    \n",
        "    \n",
        "    def training(self):\n",
        "        \n",
        "        best_steps = 0\n",
        "        total_rewards = 0\n",
        "        sum_rewards = 0\n",
        "        \n",
        "        for epoch in range(self.EPOCH):\n",
        "            obs = env.reset()\n",
        "            obs = obs.reshape([1,4])\n",
        "            done = False\n",
        "            step = 0\n",
        "            table_nos, random_nos = 0, 0 \n",
        "            while not done:\n",
        "                action, select = self.epsilon_greedy_action_selection(self.epsilon, obs)\n",
        "                next_obs, reward, done, info = env.step(action)\n",
        "                next_obs = next_obs.reshape([1,4])\n",
        "                mod_reward = self.reward_system(reward, next_obs, done, step)\n",
        "                sum_rewards += mod_reward\n",
        "                self.replay_mem.append((obs, action, mod_reward, next_obs, done))\n",
        "                obs = next_obs\n",
        "                step += 1\n",
        "                self.replay_buffer(epoch)\n",
        "                \n",
        "                if select == \"from_table\":\n",
        "                    table_nos+=1\n",
        "                elif select == \"random\":\n",
        "                    random_nos+=1\n",
        "                \n",
        "            self.epsilon = self.epsilon_reduce_exp(epoch)\n",
        "            self.update_model_handler(epoch)\n",
        "            \n",
        "            if step > best_steps:\n",
        "                best_steps = step\n",
        "                \n",
        "            if epoch % self.log_interval == 0:\n",
        "                table_per = 100 *(table_nos / (table_nos + random_nos))\n",
        "                random_per = 100 *(random_nos / (table_nos + random_nos))\n",
        "                total_rewards += sum_rewards\n",
        "                print(\"Gen:\"+str(epoch),\"random=\"+str(round(random_per,3))+\"%\",\"Neural=\"+str(round(table_per,3))+\"%\", \n",
        "                      \"epsilon=\"+str(self.epsilon), \"rewards=\"+str(sum_rewards), \"Total_rewards=\"+str(total_rewards),\n",
        "                      \"steps=\"+str(step), \"most_step=\"+str(best_steps))\n",
        "                sum_rewards = 0\n",
        "        print(\"Gen:\"+str(epoch),\"random=\"+str(round(random_per,3))+\"%\",\"Neural=\"+str(round(table_per,3))+\"%\", \"epsilon=\"+str(self.epsilon), \"rewards=\"+str(sum_rewards), \n",
        "            \"Total_rewards=\"+str(total_rewards),\"steps=\"+str(step), \"most_step=\"+str(best_steps))\n",
        "\n",
        "\n",
        "    def play(self):\n",
        "        total_reward = 0\n",
        "        obs = env.reset()\n",
        "        then = time.time()\n",
        "        step=0\n",
        "        while True:\n",
        "            obs = obs.reshape([1,4])\n",
        "            prediction = self.Q_model.predict(np.array(obs).reshape(-1, *obs.shape)/255)\n",
        "            action = np.argmax(prediction)  \n",
        "            new_obs, reward, done, info = env.step(action)\n",
        "            total_reward += self.reward_system(reward, obs, done, step)\n",
        "            obs = new_obs\n",
        "            step +=1\n",
        "            if done:\n",
        "                break\n",
        "        print(\"Time Taken=\"+str(round(then-time.time(),3))+\"s\")\n",
        "        print(\"Rewards=\"+str(total_reward))\n",
        "        env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.load_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "VKTr-p_shsc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk8ocg6NxWKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53706c2a-acb7-4b05-c799-3d30f98a7b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen:0 random=100.0% Neural=0.0% epsilon=1.0 rewards=54 Total_rewards=54 steps=18 most_step=18\n",
            "Gen:10 random=77.143% Neural=22.857% epsilon=0.9056938812736359 rewards=690 Total_rewards=744 steps=35 most_step=59\n",
            "Gen:20 random=100.0% Neural=0.0% epsilon=0.82036217630028 rewards=639 Total_rewards=1383 steps=12 most_step=59\n",
            "Gen:30 random=76.923% Neural=23.077% epsilon=0.7431508566955825 rewards=471 Total_rewards=1854 steps=13 most_step=59\n",
            "Gen:40 random=68.421% Neural=31.579% epsilon=0.6732871656213186 rewards=570 Total_rewards=2424 steps=19 most_step=59\n",
            "Gen:50 random=73.684% Neural=26.316% epsilon=0.6100718837752197 rewards=465 Total_rewards=2889 steps=19 most_step=59\n",
            "Gen:60 random=54.545% Neural=45.455% epsilon=0.5528723313691801 rewards=456 Total_rewards=3345 steps=11 most_step=59\n",
            "Gen:70 random=64.286% Neural=35.714% epsilon=0.5011160360572867 rewards=426 Total_rewards=3771 steps=14 most_step=59\n",
            "Gen:80 random=72.727% Neural=27.273% epsilon=0.4542850034401666 rewards=381 Total_rewards=4152 steps=11 most_step=59\n",
            "Gen:90 random=22.222% Neural=77.778% epsilon=0.4119105328029337 rewards=375 Total_rewards=4527 steps=9 most_step=59\n",
            "Gen:100 random=11.111% Neural=88.889% epsilon=0.37356852620089936 rewards=345 Total_rewards=4872 steps=9 most_step=59\n",
            "Gen:110 random=33.333% Neural=66.667% epsilon=0.33887524394479684 rewards=312 Total_rewards=5184 steps=9 most_step=59\n",
            "Gen:120 random=33.333% Neural=66.667% epsilon=0.3074834640049923 rewards=318 Total_rewards=5502 steps=9 most_step=59\n",
            "Gen:130 random=22.222% Neural=77.778% epsilon=0.2790790068967065 rewards=333 Total_rewards=5835 steps=9 most_step=59\n",
            "Gen:140 random=0.0% Neural=100.0% epsilon=0.25337759126613196 rewards=312 Total_rewards=6147 steps=8 most_step=59\n",
            "Gen:149 random=0.0% Neural=100.0% epsilon=0.23234430163958378 rewards=273 Total_rewards=6147 steps=15 most_step=59\n",
            "Time Taken=-0.501s\n",
            "Rewards=30\n"
          ]
        }
      ],
      "source": [
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name)\n",
        "agent = Dqn()\n",
        "agent.training()\n",
        "agent.play()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "OBXF2i_pgw5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.Q_model.save(\"/content/drive/MyDrive/Colab Notebooks/dqn_cartpole\")"
      ],
      "metadata": {
        "id": "ZoDhE0q8S_x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwbUr_P-xhoT"
      },
      "outputs": [],
      "source": [
        "import keras_tuner\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "class models:\n",
        "  def build_model(self,hp):\n",
        "      model = Sequential()\n",
        "      model.add(tf.keras.Input(shape=(1, env.observation_space.shape[0])))\n",
        "      model.add(Activation(\"relu\"))\n",
        "    # Tune the number of layers.\n",
        "      for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "          model.add(\n",
        "              layers.Dense(\n",
        "                  # Tune number of units separately.\n",
        "                  units=hp.Int(f\"units_{i}\", min_value=6, max_value=64, step=4),\n",
        "                  activation=\"softmax\"\n",
        "              )\n",
        "          )\n",
        "      if hp.Boolean(\"dropout\"):\n",
        "          model.add(layers.Dropout(rate=0.25))\n",
        "      model.add(\n",
        "          layers.Dense(\n",
        "              env.action_space.n, \n",
        "              activation=hp.Choice(\"activation\", [\"softmax\",\"linear\"]),\n",
        "              )\n",
        "          )\n",
        "      learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "      loss_function = hp.Choice(\"loss\", [\"categorical_crossentropy\",\"mse\"])\n",
        "      model.compile(\n",
        "          optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "          loss=loss_function,\n",
        "          metrics=[\"accuracy\"],\n",
        "      )\n",
        "      return model\n",
        "\n",
        "  def mysearch(self):  \n",
        "    self.mytuner = keras_tuner.RandomSearch(\n",
        "      hypermodel=self.build_model,\n",
        "      objective=\"accuracy\",\n",
        "      max_trials=5,\n",
        "      executions_per_trial=4,\n",
        "      overwrite=True,\n",
        "      directory=\"/content/drive/MyDrive/Colab Notebooks/DQN\",\n",
        "      project_name=\"DQN_cartpole\",\n",
        "    )\n",
        "    self.mytuner.search_space_summary()\n",
        "model = models()\n",
        "model.mysearch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "15ZNoLqyihuF_X8n726VdJ06TDWJ62tG0",
      "authorship_tag": "ABX9TyPmvyNpSF/L8nTL3f5XLObF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
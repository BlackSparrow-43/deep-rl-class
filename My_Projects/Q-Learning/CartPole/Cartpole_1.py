# -*- coding: utf-8 -*-
"""Q_continous_L.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ERJy_pO7GRkoZRtXUPSoOVqib5gnwIe3
"""

!pip install gym[all]  -q

import time  
import gym
import numpy as np

from gym.envs.registration import register

register(
    id='CartPole-v1',
    entry_point='gym.envs.classic_control:CartPoleEnv',
    max_episode_steps=100000,
    reward_threshold=195.0,
)

env = gym.make("CartPole-v1")
env.reset()
obs_cart_vel=[]
obs_pole_vel=[]
for i in range(10):
    action = env.action_space.sample()
    obs,reward,done,info = env.step(action)
    time.sleep(1)
    obs_cart_vel.append(obs[1])
    obs_pole_vel.append(obs[3])
    if done == True:
        break
env.close()

obs_cart_vel_max = 5
obs_cart_vel_min = -5
obs_cart_pole_max = 5
obs_cart_pole_min = -5

def bin_creation(no_of_bins):
    bin_cart_pos = np.linspace(-2.4,2.4,no_of_bins)
    bin_cart_vel = np.linspace(obs_cart_vel_min,obs_cart_vel_max,no_of_bins)
    bin_pole_pos = np.linspace(-0.10472,0.10472,no_of_bins)
    bin_pole_vel = np.linspace(obs_cart_pole_max,obs_cart_pole_min,no_of_bins)
    bins = np.array([bin_cart_pos,bin_cart_vel,bin_pole_pos,bin_pole_vel])
    return bins

bins_no = 48
bins_all = bin_creation(bins_no)

bins_all

def cont_to_dis(observation,bins):
    digitised_obs = []
    for i,obs in enumerate(observation):
        digitised_obs.append((np.digitize(obs,bins_all[i]))-1)
    return tuple(digitised_obs)

env = gym.make("CartPole-v1")
q_table_shape = (bins_no,bins_no,bins_no,bins_no,env.action_space.n)
q_table = np.zeros(q_table_shape)

q_table_shape

epoch = 50000
alpha = 0.8
gamma = .95
epsilon = 1
max_epsilon = 1
min_epsilon = .01
epsilon_end = 10000
decay_rate = .0001

def epsilon_update_linear(epsilon,epoch):
    if max_epsilon <= epoch <= epsilon_end:
        epsilon -=decay_rate
    return epsilon

def epsilon_update_greedy(Gen):
    return (min_epsilon+(max_epsilon-min_epsilon)*np.exp((-decay_rate)*Gen))

def epsilon_greedy(epsilon,q_table,state):
    random_no = np.random.random()
    if random_no > epsilon:
        action = np.argmax(q_table[state])
        select="from_table" 
    else:
        action = env.action_space.sample()
        select="random"
    return action,select

def new_q_value_system(old_q_value,reward,next_q_value):
    return old_q_value + alpha*(reward + gamma*(next_q_value - old_q_value))

def reward_system(points,reward_obs,discreted_obs,done):
    
    angle = discreted_obs[2]
    pos = discreted_obs[0]
    reward_step = reward_ang = reward_pos = 0   
    
    if 1 <= angle <= 3:
        reward_ang = -60
    elif 3 <= angle <= 5:
        reward_ang = -48
    elif 5 <= angle <= 7:
        reward_ang = -36
    elif 7 <= angle <= 9:
        reward_ang = -24
    elif 9 <= angle <= 11:
        reward_ang = -16
    elif 11 <= angle <= 13:
        reward_ang = -8
    elif 13 <= angle <= 15:
        reward_ang = -4
    elif 15 <= angle <= 17:
        reward_ang = -2
    elif 17 <= angle <= 19:
        reward_ang = 0
    elif 19 <= angle <= 21:
        reward_ang = 5
    elif 21 <= angle <= 23:
        reward_ang = 10
    elif 23 <= angle <= 25:
        reward_ang = 5
    elif 25 <= angle <= 27:
        reward_ang = 0
    elif 27 <= angle <= 29:
        reward_ang = -2
    elif 29 <= angle <= 31:
        reward_ang = -4
    elif 31 <= angle <= 33:
        reward_ang = -8
    elif 33 <= angle <= 35:
        reward_ang = -12
    elif 35 <= angle <= 37:
        reward_ang = -18
    elif 37 <= angle <= 39:
        reward_ang = -26
    elif 39 <= angle <= 41:
        reward_ang = -36
    elif 41 <= angle <= 43:
        reward_ang = -44
    elif 43 <= angle <= 45:
        reward_ang = -56
    elif 45 <= angle <= 47:
        reward_ang = -64
    else:
        reward_ang = -100


    if 0 <= pos < 4:
        reward_pos = -80
    elif 4 <= pos < 8:
        reward_pos = -40
    elif 8 <= pos < 12:
        reward_pos = -10
    elif 12 <= pos < 16:
        reward_pos = 0
    elif 16 <= pos < 20:
        reward_pos = 2
    elif 20 <= pos < 24:
        reward_pos = 5
    elif 24 <= pos < 28:
        reward_pos = 5
    elif 28 <= pos < 32:
        reward_pos = 2
    elif 32 <= pos < 36:
        reward_pos = 0
    elif 36 <= pos < 40:
        reward_pos = -10
    elif 40 <= pos < 44:
        reward_pos = -40
    elif 44 <= pos < 48:
        reward_pos = -80



    
    if done and points < 300:
        reward_step = -300
    
    return reward_ang + reward_pos + reward_obs + 1

#q_table = np.load("/content/drive/MyDrive/Deep_RL/Cartpole/Q_continous_L/q_table-2.npy")

env = gym.make("CartPole-v1")
rewards_interval = 0
rewards = []
log_interval = 1000
gen = 0
table_nos =0
random_nos = 0
total_rewards = 0
total_steps = 0

for Gen in range(epoch):
    
  state = env.reset()
  discreted_obs = cont_to_dis(state,bins_all)
  done = False
  points = 0
  steps = 0
  
  while not done:
    steps += 1 
    action,select = epsilon_greedy(epsilon,q_table,discreted_obs)
    next_state,reward,done,info = env.step(action)
    next_discreted_obs =  cont_to_dis(next_state,bins_all)
      
    old_q_value = q_table[discreted_obs+(action,)]
    next_q_estim_value = np.max(q_table[next_discreted_obs])
    reward = reward_system(points, reward, next_discreted_obs, done)
    total_rewards += reward
    new_q_value = new_q_value_system(old_q_value, reward, next_q_estim_value)
      
    q_table[discreted_obs+(action,)] = new_q_value
      
    discreted_obs = next_discreted_obs
    points += 1
    if select == "from_table":
      table_nos+=1
    elif select == "random":
      random_nos+=1

  total_steps += steps 
  epsilon = epsilon_update_greedy(Gen)
  rewards.append(total_rewards)
  rewards_interval = rewards_interval + total_rewards
  
  
  if gen%log_interval == 0:
    table_per = 100 *(table_nos / (table_nos + random_nos))
    random_per = 100 *(random_nos / (table_nos + random_nos))
    print("Gen="+str(Gen),"table_choice="+str(int(table_per)),"random_choice="+str(int(random_per)),"Last_epsisode_steps="+str(steps),"Interval_steps="+str(total_steps),"total="+str(rewards_interval),end=" ")
    rewards_interval = 0
    print("sum="+str(np.sum(rewards)),"LearningRate="+str(epsilon))
  gen = gen+1 
print("Gen="+str(Gen),"table_choice="+str(int(table_per)),"random_choice="+str(int(random_per)),"Last_epsisode_steps="+str(steps),"Interval_steps="+str(total_steps),"total="+str(rewards_interval),end=" ")
rewards_interval = 0
print("sum="+str(np.sum(rewards)),"LearningRate="+str(epsilon))
env.close()

images = []  
def show_render_4(env):
  time.sleep(.1)
  img = env.render(mode='rgb_array')
  images.append(img)

rewards = 0
points = 0 
steps = 0
observation = env.reset()
show_render_4(env)
while True:
    steps += 1
    show_render_4(env)
    discreted_obs = cont_to_dis(observation,bins_all)  # get bins
    action = np.argmax(q_table[discreted_obs])  # and chose action from the Q-Table
    observation, reward, done, info = env.step(action) # Finally perform the action
    points = points+1
    temp=reward_system(points,reward,discreted_obs,done)
    rewards += temp
    print("total_reward=",rewards)
    if done:
        break
env.close()
print("Steps="+str(steps),"Reward="+str(points))

print("Steps="+str(steps),"Reward="+str(points))

!pip install imageio imageio_ffmpeg -q

import imageio
imageio.mimsave("/content/drive/MyDrive/Deep_RL/Cartpole/Q_continous_L/cartpole-7.mp4", [np.array(img) for i, img in enumerate(images)], fps=25)

np.save("/content/drive/MyDrive/Deep_RL/Cartpole/Q_continous_L/q_table-2",q_table)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pyglet==1.5.1 
# !apt install python-opengl
# !apt install ffmpeg
# !apt install xvfb
# !pip3 install pyvirtualdisplay
# 
# # Virtual display
# from pyvirtualdisplay import Display
# 
# virtual_display = Display(visible=0, size=(1400, 900))
# virtual_display.start()

